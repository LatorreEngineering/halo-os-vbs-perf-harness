name: Halo.OS VBS Perf Harness CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      duration:
        description: 'Experiment duration (seconds)'
        required: false
        default: '120'
      scenario:
        description: 'Test scenario'
        required: false
        default: 'aeb_120kmh'
        type: choice
        options:
          - aeb_120kmh
          - lka_80kmh
          - parking

env:
  PYTHON_VERSION: '3.10'
  CACHE_VERSION: 'v5'

jobs:
  validate:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - name: Install validation tools
        run: sudo apt-get update && sudo apt-get install -y libxml2-utils shellcheck
      - name: Validate XML manifests
        run: for f in manifests/*.xml; do xmllint --noout "$f"; done || true
      - name: ShellCheck scripts
        run: for f in ci/*.sh; do shellcheck "$f"; done || true

  setup:
    runs-on: ubuntu-22.04
    needs: validate
    outputs:
      cache-key: ${{ steps.cache.outputs.key }}
    steps:
      - uses: actions/checkout@v4
      - name: Cache key
        id: cache
        run: |
          hash=$(find manifests requirements.txt -type f -exec sha256sum {} + 2>/dev/null | sha256sum | cut -d' ' -f1 || echo "nocache")
          echo "key=${{ env.CACHE_VERSION }}-${hash}" >> $GITHUB_OUTPUT
      - uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            venv/
          key: ${{ steps.cache.outputs.key }}
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install deps + venv
        run: |
          python -m venv venv
          . venv/bin/activate
          pip install --upgrade pip
          [ -f requirements.txt ] && pip install -r requirements.txt
      - name: Run setup_env.sh
        run: |
          chmod +x ci/setup_env.sh
          ./ci/setup_env.sh

  build:
    runs-on: ubuntu-22.04
    needs: setup
    steps:
      - uses: actions/checkout@v4
      - uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            venv/
          key: ${{ needs.setup.outputs.cache-key }}
      - name: Build VBSPro
        env:
          GITEE_TOKEN: ${{ secrets.GITEE_TOKEN }}
        run: |
          chmod +x ci/build_halo.sh
          ./ci/build_halo.sh
      - name: Upload build
        uses: actions/upload-artifact@v4
        with:
          name: vbspro-build
          path: build/
          retention-days: 7

  experiment:
    runs-on: ubuntu-22.04
    needs: build
    steps:
      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4
        with:
          name: vbspro-build
          path: build/
      - name: Run experiment
        env:
          LD_LIBRARY_PATH: build/install/lib:$LD_LIBRARY_PATH
        run: |
          . venv/bin/activate
          mkdir -p results
          SCENARIO="${{ github.event.inputs.scenario || 'aeb_120kmh' }}"
          DURATION="${{ github.event.inputs.duration || '120' }}"
          mkdir -p "results/$SCENARIO"
          chmod +x ci/run_experiment.sh
          ./ci/run_experiment.sh "results/$SCENARIO" "$DURATION" --scenario "$SCENARIO"
      - name: Upload traces
        uses: actions/upload-artifact@v4
        with:
          name: experiment-traces
          path: results/
          retention-days: 7

  analyze:
    runs-on: ubuntu-22.04
    needs: experiment
    steps:
      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4
        with:
          name: experiment-traces
          path: results/
      - name: Analyze traces
        run: |
          . venv/bin/activate
          for dir in results/*/; do
            f="$dir/events.jsonl"
            [ -f "$f" ] && python ci/analyze.py "$f" --output "$dir/metrics.json"
          done
      - name: Upload metrics
        uses: actions/upload-artifact@v4
        with:
          name: perf-metrics
          path: results/
          retention-days: 30

  summary:
    runs-on: ubuntu-22.04
    needs: [build, experiment, analyze]
    if: always()
    steps:
      - name: Final Status
        run: |
          success() { [[ '${{ needs.$1.result }}' == 'success' || '${{ needs.$1.result }}' == 'skipped' ]]; }
          if success build && success experiment && success analyze; then
            echo "FULL PIPELINE SUCCESS: Metrics generated!"
          else
            echo "Pipeline has failures â€” check logs."
            exit 1
          fi
